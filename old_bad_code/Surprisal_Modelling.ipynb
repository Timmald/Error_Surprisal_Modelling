{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556ef60f-064a-4b27-944a-2ad62888855f",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Welcome!\n",
    "If you have made it this far, congratulations, I am kind of shocked this reproducibility thing worked.\n",
    "\n",
    "First, we will extract the experimental info from `allsentences.txt`. Sentences, as well as their item and condition numbers. This is someone else's magic code I am borrowing, so I hope I return it eventually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac86194-d3a1-4a5d-b1cb-62f278e9eb70",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1042, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 113, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 678, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 322, in init_sockets\n",
      "    self.stdin_port = self._bind_socket(self.stdin_socket, self.stdin_port)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 252, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 228, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 300, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\\n\n",
      "2 1 As soon as the meeting ended, Paul gave the project files his assistant, eager to wrap up the day's tasks.\\n\n",
      "1 2 Unsure whether it was the right decision, Mia sold her furniture to a local dealer before moving out of her old apartment.\\n\n",
      "2 2 Unsure whether it was the right decision, Mia sold her furniture a local dealer before moving out of her old apartment.\\n\n"
     ]
    }
   ],
   "source": [
    "script_file = open(\"ad.script\", 'r')\t\t# Script file, output from eyetrack_reading.py\n",
    "\n",
    "#the Item number of the first item, as counted in the EyeTrack .script file\n",
    "#(Item number comes after the \"I\" in the trial id number)\n",
    "\n",
    "first_item = 1    \n",
    "    \n",
    "#the number of items\n",
    "\n",
    "num_items = 180\n",
    "\n",
    "#the Condition number of the first item, as counted in the EyeTrack .script file\n",
    "#(Condition number comes after the \"E\" in the trial id number)\n",
    "\n",
    "first_cond = 1\n",
    "\n",
    "#the total number of conditions, as counted in the EyeTrack .script file\n",
    "#(If multiple experiments with different numbers of conditions were included in the\n",
    "#script, then the number of conditions will be a multiple of the actual number.)\n",
    "\n",
    "num_conds = 5\n",
    "\n",
    "#define range of item numbers:\n",
    "items = range(first_item, first_item+num_items)\n",
    "conds = range(first_cond, first_cond+num_conds)\n",
    "\n",
    "#make list of lines in script_file:\n",
    "lines = script_file.readlines()\n",
    "script_file.close()\n",
    "\n",
    "#define empty list for sentences:\n",
    "sentences = []\n",
    "#define variable that keeps track of whether the next \"inline\" found is\n",
    "#a sentence and not a question:\n",
    "get = 0\n",
    "\n",
    "for line in lines:\n",
    "    if line != \"\":\n",
    "        l = line.split(\" \")\n",
    "        if 'trial' in l[0]:\n",
    "            if l[1][0]==\"E\":\n",
    "                if l[1].split('D')[1][0]=='0':\n",
    "                    cond = l[1].split('I')[0][1:]\n",
    "                    item = l[1].split('I')[1].split('D')[0]\n",
    "                    sentences.append(cond+' '+item)\n",
    "                    get = 1\n",
    "                else:\n",
    "                    get = 0\n",
    "        elif 'inline' in l:\n",
    "            if get == 1:\n",
    "                sent = line.split('|')[1]\n",
    "                s = len(sentences)-1\n",
    "                sentences[s] = sentences[s]+' '+sent\n",
    "                get = 0                \n",
    "\n",
    "# open output text file\n",
    "allsentences = open('allsentences.txt', 'w')\n",
    "\n",
    "for row in sentences:\n",
    "    allsentences.write(str(row))\n",
    "\n",
    "allsentences.close()\n",
    "!head allsentences.txt -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d5897f-0fa5-4e33-910a-2ee2455c702d",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row,item,condition,ambiguity,Sentence\n",
      "0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\"\n",
      "1,1,PREP_DAT,True,\"As soon as the meeting ended, Paul gave the project files his assistant, eager to wrap up the day's tasks.\"\n",
      "2,2,PREP_DAT,False,\"Unsure whether it was the right decision, Mia sold her furniture to a local dealer before moving out of her old apartment.\"\n"
     ]
    }
   ],
   "source": [
    "#this silly bit of code formats the sentence data to look the way some later magic code will want\n",
    "import csv\n",
    "with open(\"allsentences.txt\", \"r\") as reader:\n",
    "    lineNo = 0\n",
    "    conds = {1: (\"PREP_DAT\",True),\n",
    "             2: (\"PREP_DAT\",False),\n",
    "             3: (\"COM_SBJ\",True),\n",
    "             4: (\"COM_SBJ\",False),\n",
    "             5: (\"FILLER\", False)}\n",
    "    writer = csv.DictWriter(open(\"script_items_pivot.csv\",\"w\"),fieldnames=[\"row\",\"item\",\"condition\",\"ambiguity\",\"Sentence\"])\n",
    "    writer.writeheader()\n",
    "    for line in reader: # find the sentence, item, condition, ambiguity\n",
    "        line = line.split()\n",
    "        cnd = line[0]\n",
    "        item = line[1]\n",
    "        sentence = \" \".join(line[2:])[:-2]\n",
    "        condition = conds[int(cnd)][0]\n",
    "        ambiguous = not conds[int(cnd)][1]\n",
    "        writer.writerow({\"row\":lineNo,\"item\":item,\"condition\":condition,\"ambiguity\":ambiguous,\"Sentence\":sentence})\n",
    "        lineNo +=1\n",
    "!head script_items_pivot.csv -n 4\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e5f2d-e2b0-4f46-93ea-97c696d369ec",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "Now that we have organized the experimental trials into CSV form, we can start doing analysis! First, we collect the GPT-2 surprisals with some more magic code (this will also split the sentences word by word). Let's define functions for the magic code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381d08f1-9003-4282-ac58-9ca0c209eb6c",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(token):\n",
    "    return re.sub(\"[^a-zA-Z0-9*.,!?\\-]\", \"\", token) # filter out non-alphanumeric or punctuation characters\n",
    "\n",
    "def align(words, wordpieces, debug=False):\n",
    "    # Remove the \"not beginning of sentence\" character from the wordpieces\n",
    "    wordpieces = [clean(piece) for piece in wordpieces]\n",
    "\n",
    "    aligned = [] # list containing lists of wordpieces that make up each word\n",
    "    idx_word = 0 # idx of the next word\n",
    "    current_pieces = [] # wordpieces that don't align with the next word\n",
    "\n",
    "    for idx_piece, piece in enumerate(wordpieces):\n",
    "        if idx_word < len(words):\n",
    "            if debug: print(\"not EOS\")\n",
    "            word = words[idx_word]\n",
    "        else:\n",
    "            current_pieces += wordpieces[idx_piece:]\n",
    "            break\n",
    "\n",
    "        if debug: print(piece, word, piece == word[:len(piece)])\n",
    "\n",
    "        if piece == word[:len(piece)]:\n",
    "            # if the new wordpiece is aligned to the next word\n",
    "\n",
    "            # all current pieces belong to the current word\n",
    "            aligned.append(current_pieces)\n",
    "\n",
    "            # and the new piece belongs to the next word\n",
    "            idx_word += 1\n",
    "            current_pieces = [piece]\n",
    "        else:\n",
    "            # otherwise, the new piece belongs to the current word too\n",
    "            current_pieces.append(piece)\n",
    "\n",
    "    # at EOS, all remaining wordpieces belong to the last word\n",
    "    aligned.append(current_pieces)\n",
    "    if debug: print(\"EOS, merging the rest in: \" + \",\".join(current_pieces))\n",
    "\n",
    "    # First entry in aligned is always empty (first wordpiece should always match the first word)\n",
    "    aligned = aligned[1:]\n",
    "\n",
    "    # get the indices of the wordpiece that correspond to word boundaries (breaks)\n",
    "    breaks = [len(pieces) for pieces in aligned]\n",
    "    breaks = [0] + [sum(breaks[:i+1]) for i in range(len(breaks))]\n",
    "\n",
    "    return aligned, breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d4d5a2-fb3c-467d-bf75-f97bc2c543b7",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "260it [00:56,  4.60it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row,item,condition,ambiguity,Sentence,token,word,word_pos,sum_surprisal,sum_surprisal_base2,mean_surprisal,mean_surprisal_base2\n",
      "0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",As,As,0,10.346120834350586,14.926297220155575,10.346120834350586,14.926297220155575\n",
      "0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",soon,soon,1,4.550016403198242,6.564286100857543,4.550016403198242,6.564286100857543\n",
      "0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",as,as,2,0.005741414614021778,0.008283110391336642,0.005741414614021778,0.008283110391336642\n"
     ]
    }
   ],
   "source": [
    "#magic code to get GPT-2 surprisals\n",
    "#source: Huang et al. 2023\n",
    "!pip -q install transformers\n",
    "!pip -q install torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO option for selecting subword merges to compute\n",
    "\n",
    "# how can we combine subwords/punctuation to get one surprisal per word?\n",
    "merge_fs = {\"sum_\":sum}\n",
    "\n",
    "# Load models from HF transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_prefix_space=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#if args.cuda:\n",
    "    #device = torch.device(\"cuda\")\n",
    "    #model.to(device)\n",
    "#I commented that out for cross-platformness. SUFFER!\n",
    "\n",
    "\n",
    "in_f = open(\"script_items_pivot.csv\", \"r\")\n",
    "stims = csv.DictReader(in_f)\n",
    "\n",
    "out = []\n",
    "for stim_row in tqdm(stims):\n",
    "    inputs = tokenizer(\"<|endoftext|> \" + stim_row[\"Sentence\"], return_tensors=\"pt\")\n",
    "    ids = inputs[\"input_ids\"]\n",
    "    tokens = tokenizer.tokenize(stim_row[\"Sentence\"])\n",
    "\n",
    "    # run the model\n",
    "    outputs = model(**inputs, labels=ids)\n",
    "    logprobs = F.log_softmax(outputs.logits[0], 1)\n",
    "    \n",
    "    words = stim_row[\"Sentence\"].split()\n",
    "    piecess, breaks = align(words, tokens)\n",
    "    # get surp for each word (avgd over pieces) and write it to a new row\n",
    "    for i, (word, pieces) in enumerate(zip(words, piecess)):\n",
    "        row = stim_row.copy() # new object, not a reference\n",
    "        row[\"token\"] = \".\".join(pieces)\n",
    "        row[\"word\"] = word\n",
    "        row[\"word_pos\"] = i\n",
    "        # correct for alignment difference due to initial EOS in the model input. see get_lstm.py for details\n",
    "        surps = [-logprobs[j][ids[0][j+1]].item() for j in range(breaks[i], breaks[i+1])]\n",
    "        surps_base2 = [surp/np.log(2.0) for surp in surps]\n",
    "        for merge_fn, merge_f in merge_fs.items():\n",
    "            row[merge_fn + \"surprisal\"] = merge_f(surps)\n",
    "            #row[merge_fn + \"surprisal_base2\"] = merge_f(surps_base2)\n",
    "\n",
    "        out.append(row)\n",
    "\n",
    "with open(\"script_items_pivot.gpt2.csv\", \"w\") as out_f:\n",
    "    writer = csv.DictWriter(out_f, fieldnames = out[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(out)\n",
    "\n",
    "!head script_items_pivot.gpt2.csv -n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6217a4-e04d-4a8f-bbb9-981c83fb610c",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "Now, we can add word lengths and log frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ccf81f-63a9-4d5c-bfae-b01c2f661f8e",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row,item,condition,ambiguity,Sentence,token,word,word_pos,sum_surprisal,sum_surprisal_base2,mean_surprisal,mean_surprisal_base2,lg_freq,len\n",
      "0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",As,As,0,10.346120834350586,14.926297220155575,10.346120834350586,14.926297220155575,19.505472374673356,2\n",
      "0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",soon,soon,1,4.550016403198242,6.564286100857543,4.550016403198242,6.564286100857543,14.498288270986565,4\n",
      "0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",as,as,2,0.005741414614021778,0.008283110391336642,0.005741414614021778,0.008283110391336642,19.505472374673356,2\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import re\n",
    "\n",
    "coca_freqs = csv.DictReader(open(\"freqs_coca.csv\"))\n",
    "\n",
    "freqs = {}\n",
    "for row in coca_freqs:\n",
    "    freqs[row[\"word\"]] = int(row[\"count\"])\n",
    "\n",
    "def get_uni_freq(word):\n",
    "    return freqs.get(re.sub(\"[.,?!;:']\", \"\", word.lower()), 0)\n",
    "\n",
    "fillers = {}\n",
    "with open(\"script_items_pivot.gpt2.csv\", \"r\") as rd:\n",
    "    with open(\"words_surp_length_freq.csv\",\"w\") as wr:\n",
    "        reader = csv.DictReader(rd)\n",
    "        writer = csv.DictWriter(wr,fieldnames=list(reader.fieldnames)+[\"lg_freq\",\"len\"])\n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            fq = math.log2(get_uni_freq(row[\"word\"])) if (re.sub(\"[.,?!;:]\", \"\", row[\"word\"].lower()) in freqs and freqs[re.sub(\"[.,?!;:]\", \"\", row[\"word\"].lower())] > 0) else \"NA\"\n",
    "            row[\"lg_freq\"] = fq\n",
    "            row[\"len\"] = len(row[\"word\"])\n",
    "            writer.writerow(row)\n",
    "                \n",
    "!head words_surp_length_freq.csv -n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1e517-df56-414e-8d3e-b42906b01fe6",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "Now, we add the spillover surprisals for the last 2 words, $p_1$ and $p_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "635d0a60-4d2f-4ada-a02f-998a15acf2c1",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spillover 1!\n",
      "spillover 2\n",
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",row,item,condition,ambiguity,Sentence,token,word,word_pos,sum_surprisal,sum_surprisal_base2,mean_surprisal,mean_surprisal_base2,lg_freq,len,spillover_1,spillover_2\n",
      "0,0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",As,As,0,10.346120834350586,14.926297220155575,10.346120834350586,14.926297220155575,19.50547237467336,2,NA,NA\n",
      "1,0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",soon,soon,1,4.550016403198242,6.564286100857543,4.550016403198242,6.564286100857543,14.498288270986563,4,10.346120834350586,NA\n",
      "2,0,1,PREP_DAT,False,\"As soon as the meeting ended, Paul gave the project files to his assistant, eager to wrap up the day's tasks.\",as,as,2,0.0057414146140217,0.0082831103913366,0.0057414146140217,0.0082831103913366,19.50547237467336,2,4.550016403198242,10.346120834350586\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import nan\n",
    "\n",
    "df = pd.read_csv(\"words_surp_length_freq.csv\")\n",
    "#spillover: where the \"subject\" is the same and the \"Sentence\" but word_pos is word_pos -1 or word_pos-2\n",
    "def getspillover(row,amount):\n",
    "    if row[\"word_pos\"]-amount < 0:\n",
    "        return nan\n",
    "    #spilloverEntries = df[(df[\"subject\"]==row[\"subject\"]) & (df[\"Sentence\"]==row[\"Sentence\"]) & (df[\"word_pos\"] == row[\"word_pos\"]-amount)]\n",
    "    spilloverEntries = df[(df[\"Sentence\"]==row[\"Sentence\"]) & (df[\"word_pos\"] == row[\"word_pos\"]-amount)]\n",
    "    return spilloverEntries[\"sum_surprisal\"].iloc[0]\n",
    "\n",
    "df[\"spillover_1\"] = df.apply(lambda row: getspillover(row,1),axis=1,result_type=\"reduce\")\n",
    "print(\"spillover 1!\")\n",
    "df[\"spillover_2\"] = df.apply(lambda row: getspillover(row,2),axis=1,result_type=\"reduce\")\n",
    "print(\"spillover 2\")\n",
    "df.to_csv(\"all_predictors.csv\",na_rep=\"NA\")\n",
    "print(\"DONE\")\n",
    "!head all_predictors.csv -n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df44a3-7b8f-4e4c-acad-c0e02f1b5cd0",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "Now we have all of our predictor variables for each word, neatly in a single csv file.\n",
    "# Time to get the eye-tracking data!\n",
    "We have 2 files of interest inside `eyetracking_data`: `fp.ixs` contains first-pass times, and `ro.ixs` contains regression data. Each is a csv file outlining the item, condition, and participant, with a column for each word.\n",
    "\n",
    "first, we need to replace all of the empty columns in `ro.ixs` and all of the 0 ms GP times in `fp.ixs` with `NA`. This will help R deal with the NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bdbb4d5-5db8-43aa-bc4d-8105cee8bbea",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   seq  subj  item  cond   R1    R2    R3    R4    R5    R6  ...   R14   R15  \\\n",
      "0  170     1     1     5   62   343   515  <NA>   411   514  ...   258    47   \n",
      "1  349     1     2     5  157   235   256  <NA>   180  <NA>  ...   352   673   \n",
      "2  128     1     3     5  231  <NA>   528  <NA>   302   476  ...   269  <NA>   \n",
      "3  347     1     4     5  215   388   174  <NA>   322   364  ...   546   201   \n",
      "4  114     1     5     5  263  <NA>  <NA>   416  <NA>   286  ...  <NA>   358   \n",
      "\n",
      "    R16   R17   R18   R19   R20   R21   R22   R23  \n",
      "0  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
      "1  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
      "2  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
      "3  <NA>   349   321  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
      "4  <NA>   324   267  <NA>   341  <NA>  <NA>  <NA>  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>subj</th>\n",
       "      <th>item</th>\n",
       "      <th>cond</th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "      <th>R3</th>\n",
       "      <th>R4</th>\n",
       "      <th>R5</th>\n",
       "      <th>R6</th>\n",
       "      <th>...</th>\n",
       "      <th>R14</th>\n",
       "      <th>R15</th>\n",
       "      <th>R16</th>\n",
       "      <th>R17</th>\n",
       "      <th>R18</th>\n",
       "      <th>R19</th>\n",
       "      <th>R20</th>\n",
       "      <th>R21</th>\n",
       "      <th>R22</th>\n",
       "      <th>R23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>349</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq  subj  item  cond  R1    R2    R3    R4    R5    R6  ...   R14   R15  \\\n",
       "0  170     1     1     5   0     0     0  <NA>     0     0  ...     1  <NA>   \n",
       "1  349     1     2     5   0     0     1  <NA>     0  <NA>  ...     0     1   \n",
       "2  128     1     3     5   0  <NA>     0  <NA>     0     0  ...     1  <NA>   \n",
       "3  347     1     4     5   0     0     1  <NA>     0     0  ...     0     0   \n",
       "4  114     1     5     5   0  <NA>  <NA>     1  <NA>     0  ...  <NA>     1   \n",
       "\n",
       "    R16   R17   R18   R19   R20   R21   R22   R23  \n",
       "0  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "1  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "2  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "3  <NA>     0     0  <NA>  <NA>  <NA>  <NA>  <NA>  \n",
       "4  <NA>     1     0  <NA>     1  <NA>  <NA>  <NA>  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "\n",
    "\n",
    "\n",
    "#replace 0 FP times with NA\n",
    "df = pd.read_csv(\"eyetracking_data/fp.ixs\")\n",
    "df = df.astype(pd.Int64Dtype(),copy=True)\n",
    "df = df.replace(0,nan)\n",
    "df.to_csv(\"eyetracking_data/fp_clean.ixs\",na_rep=\"NA\")\n",
    "\n",
    "#pandas should autointerepret the ,, columns of RO as NA\n",
    "print(df.head())\n",
    "\n",
    "ro_df = pd.read_csv(\"eyetracking_data/ro.ixs\")\n",
    "ro_df = ro_df.astype(pd.Int64Dtype(),copy=True)\n",
    "ro_df.to_csv(\"eyetracking_data/ro_clean.ixs\",na_rep=\"NA\")\n",
    "ro_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b81f9-803a-4d8a-bc5f-25e7d3afde2d",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "With our eyetracking data cleaned, we can join it with the rest of our data. Because I was originally foolish and I do not want to change my old code to much, we are going to do this wide-form first, then turn it into the long-form table we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb19fecf-f0af-482f-84c7-2ef842eb3be4",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████▊| 3900/3973 [05:04<00:05, 12.81it/s]\n",
      "\n",
      "  3%|▎         | 100/3973 [00:03<01:59, 32.43it/s]\u001b[A\n",
      "  5%|▌         | 200/3973 [00:06<01:54, 32.97it/s]\u001b[A\n",
      "  8%|▊         | 300/3973 [00:09<01:50, 33.27it/s]\u001b[A\n",
      " 10%|█         | 400/3973 [00:12<01:47, 33.14it/s]\u001b[A\n",
      " 13%|█▎        | 500/3973 [00:15<01:46, 32.70it/s]\u001b[A\n",
      " 15%|█▌        | 600/3973 [00:18<01:43, 32.69it/s]\u001b[A\n",
      " 18%|█▊        | 700/3973 [00:21<01:40, 32.43it/s]\u001b[A\n",
      " 20%|██        | 800/3973 [00:24<01:36, 32.95it/s]\u001b[A\n",
      " 23%|██▎       | 900/3973 [00:27<01:34, 32.44it/s]\u001b[A\n",
      " 25%|██▌       | 1000/3973 [00:30<01:33, 31.92it/s]\u001b[A\n",
      " 28%|██▊       | 1100/3973 [00:33<01:28, 32.39it/s]\u001b[A\n",
      " 30%|███       | 1200/3973 [00:36<01:25, 32.57it/s]\u001b[A\n",
      " 33%|███▎      | 1300/3973 [00:39<01:21, 32.68it/s]\u001b[A\n",
      " 35%|███▌      | 1400/3973 [00:42<01:18, 32.79it/s]\u001b[A\n",
      " 38%|███▊      | 1500/3973 [00:45<01:16, 32.50it/s]\u001b[A\n",
      " 40%|████      | 1600/3973 [00:49<01:13, 32.50it/s]\u001b[A\n",
      " 43%|████▎     | 1700/3973 [00:52<01:09, 32.60it/s]\u001b[A\n",
      " 45%|████▌     | 1800/3973 [00:55<01:06, 32.89it/s]\u001b[A\n",
      " 48%|████▊     | 1900/3973 [00:58<01:02, 32.97it/s]\u001b[A\n",
      " 50%|█████     | 2000/3973 [01:01<01:00, 32.69it/s]\u001b[A\n",
      " 53%|█████▎    | 2100/3973 [01:04<00:57, 32.81it/s]\u001b[A\n",
      " 55%|█████▌    | 2200/3973 [01:07<00:53, 33.03it/s]\u001b[A\n",
      " 58%|█████▊    | 2300/3973 [01:10<00:50, 33.06it/s]\u001b[A\n",
      " 60%|██████    | 2400/3973 [01:13<00:47, 33.03it/s]\u001b[A\n",
      " 63%|██████▎   | 2500/3973 [01:16<00:44, 32.88it/s]\u001b[A\n",
      " 65%|██████▌   | 2600/3973 [01:19<00:41, 32.94it/s]\u001b[A\n",
      " 68%|██████▊   | 2700/3973 [01:22<00:39, 32.22it/s]\u001b[A\n",
      " 70%|███████   | 2800/3973 [01:25<00:36, 32.54it/s]\u001b[A\n",
      " 73%|███████▎  | 2900/3973 [01:28<00:33, 32.23it/s]\u001b[A\n",
      " 76%|███████▌  | 3000/3973 [01:32<00:30, 31.73it/s]\u001b[A\n",
      " 78%|███████▊  | 3100/3973 [01:35<00:27, 31.18it/s]\u001b[A\n",
      " 81%|████████  | 3200/3973 [01:38<00:24, 31.39it/s]\u001b[A\n",
      " 83%|████████▎ | 3300/3973 [01:43<00:24, 27.51it/s]\u001b[A\n",
      " 86%|████████▌ | 3400/3973 [01:46<00:20, 28.46it/s]\u001b[A\n",
      " 88%|████████▊ | 3500/3973 [01:49<00:16, 28.71it/s]\u001b[A\n",
      " 91%|█████████ | 3600/3973 [01:52<00:12, 29.78it/s]\u001b[A\n",
      " 93%|█████████▎| 3700/3973 [01:55<00:08, 30.77it/s]\u001b[A\n",
      " 96%|█████████▌| 3800/3973 [01:58<00:05, 31.58it/s]\u001b[A\n",
      " 98%|█████████▊| 3900/3973 [02:02<00:02, 30.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from numpy import nan\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"all_predictors.csv\")\n",
    "pbar = tqdm(total=len(df))\n",
    "\n",
    "i = 0\n",
    "#if you are wondering why I did it this way, just know my old code was even more foolhardy\n",
    "def add_eye_data_cols(row):\n",
    "    global i\n",
    "    i += 1\n",
    "    item = row[\"item\"]\n",
    "    wordPos1 = row[\"word_pos\"]+1\n",
    "    rodf = pd.read_csv(\"eyetracking_data/ro_clean.ixs\")\n",
    "    rodf = rodf[rodf[\"item\"]==item]\n",
    "    rodf = rodf[pd.notna(rodf[f\"R{wordPos1}\"])]\n",
    "\n",
    "    subj = rodf[\"subj\"].astype(pd.Int16Dtype(),copy=True).apply(lambda val:f\"ro_{val}\")\n",
    "    row = pd.concat([row,pd.Series(list(rodf[f\"R{wordPos1}\"]),list(subj),dtype=pd.Int16Dtype())])\n",
    "    \n",
    "    fpdf = pd.read_csv(\"eyetracking_data/fp_clean.ixs\")\n",
    "    fpdf = fpdf[fpdf[\"item\"]==item]\n",
    "    fpdf = fpdf[pd.notna(fpdf[f\"R{wordPos1}\"])]\n",
    "\n",
    "    subj = fpdf[\"subj\"].astype(pd.Int16Dtype(),copy=True).apply(lambda val:f\"fp_{val}\")\n",
    "    row = pd.concat([row,pd.Series(list(fpdf[f\"R{wordPos1}\"]),list(subj),dtype=pd.Int16Dtype())])\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        pbar.update(100)\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(add_eye_data_cols,1)\n",
    "#for subj in range(1,77):\n",
    "    #df = df.astype({f\"ro_{subj}\":pd.Int32Dtype(),f\"fp_{subj}\":pd.Int32Dtype()})\n",
    "\n",
    "#0-4 fakerow,Sentence,Unnamed: 0, ambiguity, condition\n",
    "fpidxs = list(range(4,81))\n",
    "#82-86 item,len,lg_freq,mean_surp,meansurp2\n",
    "roidxs = list(range(86,163))\n",
    "#164-171 row,spillover1,spillover2,sumsurp,sumsurp2,token,word,pos\n",
    "df = df.iloc[:,[163,3,81,2,0,169,170,166,164,165,82,83]+fpidxs+roidxs]\n",
    "\n",
    "\n",
    "df.to_csv(\"wideform_data.csv\",na_rep=\"NA\")\n",
    "print(\"DONE\")\n",
    "    #takes a series row, gets eyetracking data, adds shit to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5572c-3b84-42db-a007-a78535ea0c17",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "Now, we will use the magic of R to turn this mess into a longform table as god intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fec7a5-8928-46b2-a846-5dc6bf18fafc",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda update r-rlang -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92f71d2-a092-4956-a9bb-c3b50539f6f1",
   "metadata": {
    "kernel": "R",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trying URL 'https://cran.r-project.org/src/contrib/rlang_1.1.6.tar.gz'\n",
      "Content type 'application/x-gzip' length 767928 bytes (749 KB)\n",
      "==================================================\n",
      "downloaded 749 KB\n",
      "\n",
      "* installing *source* package ‘rlang’ ...\n",
      "** package ‘rlang’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n",
      "** libs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x86_64-conda-linux-gnu-cc -I\"/opt/conda/lib/R/include\" -DNDEBUG -I./rlang/  -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib  -fvisibility=hidden -fpic  -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c capture.c -o capture.o\n",
      "x86_64-conda-linux-gnu-cc -I\"/opt/conda/lib/R/include\" -DNDEBUG -I./rlang/  -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib  -fvisibility=hidden -fpic  -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c internal.c -o internal.o\n",
      "x86_64-conda-linux-gnu-cc -I\"/opt/conda/lib/R/include\" -DNDEBUG -I./rlang/  -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib  -fvisibility=hidden -fpic  -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c rlang.c -o rlang.o\n",
      "x86_64-conda-linux-gnu-cc -I\"/opt/conda/lib/R/include\" -DNDEBUG -I./rlang/  -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib  -fvisibility=hidden -fpic  -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c version.c -o version.o\n",
      "x86_64-conda-linux-gnu-cc -shared -L/opt/conda/lib/R/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/opt/conda/lib -Wl,-rpath-link,/opt/conda/lib -L/opt/conda/lib -o rlang.so capture.o internal.o rlang.o version.o -L/opt/conda/lib/R/lib -lR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing to /opt/conda/lib/R/library/00LOCK-rlang/00new/rlang/libs\n",
      "** R\n",
      "** inst\n",
      "** byte-compile and prepare package for lazy loading\n",
      "** help\n",
      "*** installing help indices\n",
      "*** copying figures\n",
      "** building package indices\n",
      "** testing if installed package can be loaded from temporary location\n",
      "** checking absolute paths in shared objects and dynamic libraries\n",
      "** testing if installed package can be loaded from final location\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "** testing if installed package keeps a record of temporary installation path\n",
      "* DONE (rlang)\n",
      "\n",
      "The downloaded source packages are in\n",
      "\t‘/tmp/RtmpqFOWQ4/downloaded_packages’\n",
      "trying URL 'https://cloud.r-project.org/src/contrib/tidyr_1.3.1.tar.gz'\n",
      "Content type 'application/x-gzip' length 809058 bytes (790 KB)\n",
      "==================================================\n",
      "downloaded 790 KB\n",
      "\n",
      "* installing *source* package ‘tidyr’ ...\n",
      "** package ‘tidyr’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n",
      "** libs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG  -I'/opt/conda/lib/R/library/cpp11/include' -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c cpp11.cpp -o cpp11.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG  -I'/opt/conda/lib/R/library/cpp11/include' -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c melt.cpp -o melt.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG  -I'/opt/conda/lib/R/library/cpp11/include' -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c simplifyPieces.cpp -o simplifyPieces.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -shared -L/opt/conda/lib/R/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/opt/conda/lib -Wl,-rpath-link,/opt/conda/lib -L/opt/conda/lib -o tidyr.so cpp11.o melt.o simplifyPieces.o -L/opt/conda/lib/R/lib -lR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing to /opt/conda/lib/R/library/00LOCK-tidyr/00new/tidyr/libs\n",
      "** R\n",
      "** data\n",
      "*** moving datasets to lazyload DB\n",
      "** inst\n",
      "** byte-compile and prepare package for lazy loading\n",
      "** help\n",
      "*** installing help indices\n",
      "*** copying figures\n",
      "** building package indices\n",
      "** installing vignettes\n",
      "** testing if installed package can be loaded from temporary location\n",
      "** checking absolute paths in shared objects and dynamic libraries\n",
      "** testing if installed package can be loaded from final location\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "** testing if installed package keeps a record of temporary installation path\n",
      "* DONE (tidyr)\n",
      "\n",
      "The downloaded source packages are in\n",
      "\t‘/tmp/RtmpqFOWQ4/downloaded_packages’\n",
      "trying URL 'https://cloud.r-project.org/src/contrib/dplyr_1.1.4.tar.gz'\n",
      "Content type 'application/x-gzip' length 1207521 bytes (1.2 MB)\n",
      "==================================================\n",
      "downloaded 1.2 MB\n",
      "\n",
      "* installing *source* package ‘dplyr’ ...\n",
      "** package ‘dplyr’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n",
      "** libs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c chop.cpp -o chop.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c filter.cpp -o filter.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c funs.cpp -o funs.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c group_by.cpp -o group_by.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c group_data.cpp -o group_data.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c imports.cpp -o imports.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c init.cpp -o init.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c mask.cpp -o mask.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c mutate.cpp -o mutate.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c reconstruct.cpp -o reconstruct.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c slice.cpp -o slice.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -I\"/opt/conda/lib/R/include\" -DNDEBUG   -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /opt/conda/include -I/opt/conda/include -Wl,-rpath-link,/opt/conda/lib   -fpic  -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /opt/conda/include -fdebug-prefix-map=/home/conda/feedstock_root/build_artifacts/r-base-split_1671440533574/work=/usr/local/src/conda/r-base-4.2.2 -fdebug-prefix-map=/opt/conda=/usr/local/src/conda-prefix  -c summarise.cpp -o summarise.o\n",
      "x86_64-conda-linux-gnu-c++ -std=gnu++14 -shared -L/opt/conda/lib/R/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/opt/conda/lib -Wl,-rpath-link,/opt/conda/lib -L/opt/conda/lib -o dplyr.so chop.o filter.o funs.o group_by.o group_data.o imports.o init.o mask.o mutate.o reconstruct.o slice.o summarise.o -L/opt/conda/lib/R/lib -lR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing to /opt/conda/lib/R/library/00LOCK-dplyr/00new/dplyr/libs\n",
      "** R\n",
      "** data\n",
      "*** moving datasets to lazyload DB\n",
      "** inst\n",
      "** byte-compile and prepare package for lazy loading\n",
      "** help\n",
      "*** installing help indices\n",
      "*** copying figures\n",
      "** building package indices\n",
      "** installing vignettes\n",
      "** testing if installed package can be loaded from temporary location\n",
      "** checking absolute paths in shared objects and dynamic libraries\n",
      "** testing if installed package can be loaded from final location\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "** testing if installed package keeps a record of temporary installation path\n",
      "* DONE (dplyr)\n",
      "\n",
      "The downloaded source packages are in\n",
      "\t‘/tmp/RtmpqFOWQ4/downloaded_packages’\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'fp_1'</li><li>'fp_10'</li><li>'fp_11'</li><li>'fp_12'</li><li>'fp_13'</li><li>'fp_14'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'fp\\_1'\n",
       "\\item 'fp\\_10'\n",
       "\\item 'fp\\_11'\n",
       "\\item 'fp\\_12'\n",
       "\\item 'fp\\_13'\n",
       "\\item 'fp\\_14'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'fp_1'\n",
       "2. 'fp_10'\n",
       "3. 'fp_11'\n",
       "4. 'fp_12'\n",
       "5. 'fp_13'\n",
       "6. 'fp_14'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"fp_1\"  \"fp_10\" \"fp_11\" \"fp_12\" \"fp_13\" \"fp_14\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'fp_1'</li><li>'fp_10'</li><li>'fp_11'</li><li>'fp_12'</li><li>'fp_13'</li><li>'fp_14'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'fp\\_1'\n",
       "\\item 'fp\\_10'\n",
       "\\item 'fp\\_11'\n",
       "\\item 'fp\\_12'\n",
       "\\item 'fp\\_13'\n",
       "\\item 'fp\\_14'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'fp_1'\n",
       "2. 'fp_10'\n",
       "3. 'fp_11'\n",
       "4. 'fp_12'\n",
       "5. 'fp_13'\n",
       "6. 'fp_14'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"fp_1\"  \"fp_10\" \"fp_11\" \"fp_12\" \"fp_13\" \"fp_14\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mJoining with `by = join_by(X, row, condition, item, ambiguity, Sentence, word,\n",
      "word_pos, sum_surprisal, spillover_1, spillover_2, len, lg_freq, subject)`\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"rlang\")\n",
    "install.packages(\"tidyr\", repos = \"https://cloud.r-project.org\")\n",
    "install.packages(\"dplyr\", repos = \"https://cloud.r-project.org\")\n",
    "library(\"tidyr\")\n",
    "library(\"dplyr\")\n",
    "orig_data <- read.csv(\"wideform_data.csv\")\n",
    "fp_data <- orig_data %>% pivot_longer(cols=14:90,names_to=\"subject\",values_to=\"fp\")#0 idx?\n",
    "fp_data[14:90]<-NULL\n",
    "#fp_data[18:21]<-NULL\n",
    "ro_data <- orig_data %>% pivot_longer(cols=91:167,names_to=\"subject\",values_to=\"ro\")#0 idx?\n",
    "ro_data[14:90]<-NULL\n",
    "#ro_data[18:21]<-NULL\n",
    "ro_data$subject<-gsub(\"ro\",\"fp\",ro_data$subject)\n",
    "head(fp_data$subject)\n",
    "head(ro_data$subject)\n",
    "\n",
    "\n",
    "merged_data <- right_join(fp_data,ro_data)\n",
    "write.csv(merged_data,\"data.pivot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd6c4df8-504c-4505-83d0-bec0bec81d7d",
   "metadata": {
    "kernel": "R",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependency ‘nloptr’\n",
      "\n",
      "\n",
      "trying URL 'https://cran.r-project.org/src/contrib/nloptr_2.2.1.tar.gz'\n",
      "Content type 'application/x-gzip' length 2253853 bytes (2.1 MB)\n",
      "==================================================\n",
      "downloaded 2.1 MB\n",
      "\n",
      "trying URL 'https://cran.r-project.org/src/contrib/lme4_1.1-37.tar.gz'\n",
      "Content type 'application/x-gzip' length 3313529 bytes (3.2 MB)\n",
      "==================================================\n",
      "downloaded 3.2 MB\n",
      "\n",
      "* installing *source* package ‘nloptr’ ...\n",
      "** package ‘nloptr’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking whether the C++ compiler works... yes\n",
      "checking for C++ compiler default output file name... a.out\n",
      "checking for suffix of executables... \n",
      "checking whether we are cross compiling... no\n",
      "checking for suffix of object files... o\n",
      "checking whether the compiler supports GNU C++... yes\n",
      "checking whether x86_64-conda-linux-gnu-c++ -std=gnu++14 accepts -g... yes\n",
      "checking for x86_64-conda-linux-gnu-c++ -std=gnu++14 option to enable C++11 features... none needed\n",
      "checking how to run the C++ preprocessor... x86_64-conda-linux-gnu-c++ -std=gnu++14 -E\n",
      "checking whether the compiler supports GNU C++... (cached) yes\n",
      "checking whether x86_64-conda-linux-gnu-c++ -std=gnu++14 accepts -g... (cached) yes\n",
      "checking for x86_64-conda-linux-gnu-c++ -std=gnu++14 option to enable C++11 features... (cached) none needed\n",
      "checking for pkg-config... no\n",
      "checking for cmake... no\n",
      "\n",
      "------------------ CMAKE NOT FOUND --------------------\n",
      "\n",
      "CMake was not found on the PATH. Please install CMake:\n",
      "\n",
      " - sudo yum install cmake          (Fedora/CentOS; inside a terminal)\n",
      " - sudo apt install cmake          (Debian/Ubuntu; inside a terminal).\n",
      " - sudo pacman -S cmake            (Arch Linux; inside a terminal).\n",
      " - brew install --cask cmake       (MacOS; inside a terminal with Homebrew)\n",
      " - sudo port install cmake         (MacOS; inside a terminal with MacPorts)\n",
      "\n",
      "Alternatively install CMake from: <https://cmake.org/>\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in install.packages(\"lme4\"):\n",
      "“installation of package ‘nloptr’ had non-zero exit status”\n",
      "ERROR: configuration failed for package ‘nloptr’\n",
      "* removing ‘/opt/conda/lib/R/library/nloptr’\n",
      "Warning message in install.packages(\"lme4\"):\n",
      "“installation of package ‘lme4’ had non-zero exit status”\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "ERROR: dependency ‘nloptr’ is not available for package ‘lme4’\n",
      "* removing ‘/opt/conda/lib/R/library/lme4’\n",
      "\n",
      "The downloaded source packages are in\n",
      "\t‘/tmp/RtmpqFOWQ4/downloaded_packages’\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in lmer(data = filler_data, fp ~ sum_surprisal + spillover_1 + spillover_2 + : could not find function \"lmer\"\n",
     "output_type": "error",
     "traceback": [
      "Error in lmer(data = filler_data, fp ~ sum_surprisal + spillover_1 + spillover_2 + : could not find function \"lmer\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"lme4\")\n",
    "filler_data = read.csv(\"data.pivot.csv\")\n",
    "filler_data$subject <- as.factor(filler_data$subject)\n",
    "filler.model_surp = lmer(data=filler_data,\n",
    "                           fp ~ sum_surprisal+spillover_1+spillover_2+word_pos+lg_freq*len+(1 | subject),\n",
    "control=lmerControl(optimizer=\"bobyqa\",optCtrl=list(maxfun=2e5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39eb319-2e32-44fa-bfb6-51d501d0a69e",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"apt install cmake -y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7fc1a-2180-4557-ba12-9e84cdc4518a",
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     {
      "name": "ipython",
      "version": 3
     }
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ]
   ],
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
